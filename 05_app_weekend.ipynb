{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhijiehuang/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from flags import FLAGS, unparsed\n",
    "import numpy as np\n",
    "from db.conn_db import db,cursor,engine,truncate_table,data_from_mysql,dev_id_train\n",
    "import operator  \n",
    "from functools import reduce\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from data_preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-06 09:06:55,556 - DEBUG - data_preprocessing.py:155 - Index(['age', 'device_id', 'sex', 'n_class', 'type_no_w', '01', '02', '02f',\n",
      "       '1100', '1105',\n",
      "       ...\n",
      "       '零售o2o', '音乐学习', '音乐播放器', '音乐编辑', '音乐节奏', '项目管理', '飞行射击', '食谱', '餐饮品牌',\n",
      "       '驾驶学习'],\n",
      "      dtype='object', length=875)\n",
      "2018-10-06 09:06:55,558 - DEBUG - data_preprocessing.py:156 - (50000, 875)\n",
      "2018-10-06 09:06:55,997 - DEBUG - data_preprocessing.py:178 - Index(['n_class', 'type_no_w', '01', '02', '02f', '1100', '1105', '1107', '1s',\n",
      "       '2013022',\n",
      "       ...\n",
      "       '零售o2o', '音乐学习', '音乐播放器', '音乐编辑', '音乐节奏', '项目管理', '飞行射击', '食谱', '餐饮品牌',\n",
      "       '驾驶学习'],\n",
      "      dtype='object', length=872)\n",
      "2018-10-06 09:06:55,998 - DEBUG - data_preprocessing.py:179 - (50000, 872)\n",
      "2018-10-06 09:06:55,999 - DEBUG - data_preprocessing.py:180 -    n_class  type_no_w   01   02  02f  1100  1105  1107   1s  2013022  ...   \\\n",
      "0        3   0.599425  0.0  0.0  0.0   0.0   0.0   0.0  0.0      0.0  ...    \n",
      "1        5   0.667416  0.0  0.0  0.0   0.0   0.0   0.0  0.0      0.0  ...    \n",
      "\n",
      "   零售o2o  音乐学习  音乐播放器  音乐编辑  音乐节奏  项目管理  飞行射击  食谱  餐饮品牌  驾驶学习  \n",
      "0      0     0      0     0     0     0     0   0     0     0  \n",
      "1      0     0      0     0     0     0     0   0     0     0  \n",
      "\n",
      "[2 rows x 872 columns]\n",
      "/home/zhijiehuang/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "########## XGBOOST ##########\n",
    "\n",
    "params = {}\n",
    "params['booster'] = 'gblinear'\n",
    "params['objective'] = \"multi:softprob\"\n",
    "params['eval_metric'] = 'mlogloss'\n",
    "params['eta'] = 0.1\n",
    "params['num_class'] = 22\n",
    "params['lambda'] = 3\n",
    "params['alpha'] = 2\n",
    "\n",
    "train_save = gdbt_data_get_train('n_class')\n",
    "y_train = train_save['n_class']\n",
    "targetencoder = LabelEncoder().fit(train_save.n_class)\n",
    "y = targetencoder.transform(train_save.n_class)\n",
    "train_save.drop('n_class',axis=1,inplace=True)\n",
    "X_train = train_save\n",
    "\n",
    "kf = list(StratifiedKFold(y, n_folds=10, shuffle=True, random_state=4242))[0]\n",
    "\n",
    "X_train_part, X_val, y_train_part, y_val = train_test_split(X_train, y_train, train_size = 0.8,random_state = 4242)\n",
    "\n",
    "d_train = xgb.DMatrix(X_train_part, label=y_train_part)\n",
    "d_valid = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'eval')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:3.05112\teval-mlogloss:3.05186\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mlogloss:3.02131\teval-mlogloss:3.02269\n",
      "[2]\ttrain-mlogloss:2.9985\teval-mlogloss:3.00045\n",
      "[3]\ttrain-mlogloss:2.98044\teval-mlogloss:2.9829\n",
      "[4]\ttrain-mlogloss:2.96607\teval-mlogloss:2.96899\n",
      "[5]\ttrain-mlogloss:2.95444\teval-mlogloss:2.95776\n",
      "[6]\ttrain-mlogloss:2.94495\teval-mlogloss:2.94862\n",
      "[7]\ttrain-mlogloss:2.93715\teval-mlogloss:2.94115\n",
      "[8]\ttrain-mlogloss:2.93072\teval-mlogloss:2.935\n",
      "[9]\ttrain-mlogloss:2.92538\teval-mlogloss:2.92993\n",
      "[10]\ttrain-mlogloss:2.92093\teval-mlogloss:2.92573\n",
      "[11]\ttrain-mlogloss:2.91721\teval-mlogloss:2.92222\n",
      "[12]\ttrain-mlogloss:2.91408\teval-mlogloss:2.9193\n",
      "[13]\ttrain-mlogloss:2.91145\teval-mlogloss:2.91685\n",
      "[14]\ttrain-mlogloss:2.90923\teval-mlogloss:2.91479\n",
      "[15]\ttrain-mlogloss:2.90734\teval-mlogloss:2.91305\n",
      "[16]\ttrain-mlogloss:2.90573\teval-mlogloss:2.91158\n",
      "[17]\ttrain-mlogloss:2.90436\teval-mlogloss:2.91033\n",
      "[18]\ttrain-mlogloss:2.90318\teval-mlogloss:2.90927\n",
      "[19]\ttrain-mlogloss:2.90217\teval-mlogloss:2.90836\n",
      "[20]\ttrain-mlogloss:2.9013\teval-mlogloss:2.90759\n",
      "[21]\ttrain-mlogloss:2.90054\teval-mlogloss:2.90692\n",
      "[22]\ttrain-mlogloss:2.89988\teval-mlogloss:2.90633\n",
      "[23]\ttrain-mlogloss:2.89931\teval-mlogloss:2.90583\n",
      "[24]\ttrain-mlogloss:2.8988\teval-mlogloss:2.90539\n",
      "[25]\ttrain-mlogloss:2.89836\teval-mlogloss:2.905\n",
      "[26]\ttrain-mlogloss:2.89796\teval-mlogloss:2.90465\n",
      "[27]\ttrain-mlogloss:2.89761\teval-mlogloss:2.90434\n",
      "[28]\ttrain-mlogloss:2.8973\teval-mlogloss:2.90407\n",
      "[29]\ttrain-mlogloss:2.89701\teval-mlogloss:2.90382\n",
      "[30]\ttrain-mlogloss:2.89676\teval-mlogloss:2.90359\n",
      "[31]\ttrain-mlogloss:2.89652\teval-mlogloss:2.90339\n",
      "[32]\ttrain-mlogloss:2.89631\teval-mlogloss:2.9032\n",
      "[33]\ttrain-mlogloss:2.89611\teval-mlogloss:2.90302\n",
      "[34]\ttrain-mlogloss:2.89593\teval-mlogloss:2.90285\n",
      "[35]\ttrain-mlogloss:2.89576\teval-mlogloss:2.9027\n",
      "[36]\ttrain-mlogloss:2.8956\teval-mlogloss:2.90256\n",
      "[37]\ttrain-mlogloss:2.89545\teval-mlogloss:2.90242\n",
      "[38]\ttrain-mlogloss:2.89531\teval-mlogloss:2.90229\n",
      "[39]\ttrain-mlogloss:2.89518\teval-mlogloss:2.90217\n",
      "[40]\ttrain-mlogloss:2.89505\teval-mlogloss:2.90205\n",
      "[41]\ttrain-mlogloss:2.89493\teval-mlogloss:2.90194\n",
      "[42]\ttrain-mlogloss:2.89482\teval-mlogloss:2.90184\n",
      "[43]\ttrain-mlogloss:2.89471\teval-mlogloss:2.90174\n",
      "[44]\ttrain-mlogloss:2.89461\teval-mlogloss:2.90164\n",
      "[45]\ttrain-mlogloss:2.89451\teval-mlogloss:2.90155\n",
      "[46]\ttrain-mlogloss:2.89442\teval-mlogloss:2.90146\n",
      "[47]\ttrain-mlogloss:2.89432\teval-mlogloss:2.90137\n",
      "[48]\ttrain-mlogloss:2.89424\teval-mlogloss:2.90129\n",
      "[49]\ttrain-mlogloss:2.89415\teval-mlogloss:2.90121\n",
      "[50]\ttrain-mlogloss:2.89407\teval-mlogloss:2.90114\n",
      "[51]\ttrain-mlogloss:2.89399\teval-mlogloss:2.90107\n",
      "[52]\ttrain-mlogloss:2.89391\teval-mlogloss:2.901\n",
      "[53]\ttrain-mlogloss:2.89384\teval-mlogloss:2.90093\n",
      "[54]\ttrain-mlogloss:2.89377\teval-mlogloss:2.90087\n",
      "[55]\ttrain-mlogloss:2.8937\teval-mlogloss:2.90081\n",
      "[56]\ttrain-mlogloss:2.89363\teval-mlogloss:2.90075\n",
      "[57]\ttrain-mlogloss:2.89357\teval-mlogloss:2.90069\n",
      "[58]\ttrain-mlogloss:2.8935\teval-mlogloss:2.90063\n",
      "[59]\ttrain-mlogloss:2.89344\teval-mlogloss:2.90058\n",
      "[60]\ttrain-mlogloss:2.89338\teval-mlogloss:2.90053\n",
      "[61]\ttrain-mlogloss:2.89332\teval-mlogloss:2.90048\n",
      "[62]\ttrain-mlogloss:2.89326\teval-mlogloss:2.90043\n",
      "[63]\ttrain-mlogloss:2.89321\teval-mlogloss:2.90038\n",
      "[64]\ttrain-mlogloss:2.89316\teval-mlogloss:2.90034\n",
      "[65]\ttrain-mlogloss:2.8931\teval-mlogloss:2.90029\n",
      "[66]\ttrain-mlogloss:2.89305\teval-mlogloss:2.90025\n",
      "[67]\ttrain-mlogloss:2.893\teval-mlogloss:2.90021\n",
      "[68]\ttrain-mlogloss:2.89295\teval-mlogloss:2.90017\n",
      "[69]\ttrain-mlogloss:2.8929\teval-mlogloss:2.90013\n",
      "[70]\ttrain-mlogloss:2.89285\teval-mlogloss:2.90009\n",
      "[71]\ttrain-mlogloss:2.89281\teval-mlogloss:2.90005\n",
      "[72]\ttrain-mlogloss:2.89276\teval-mlogloss:2.90002\n",
      "[73]\ttrain-mlogloss:2.89272\teval-mlogloss:2.89998\n",
      "[74]\ttrain-mlogloss:2.89268\teval-mlogloss:2.89994\n",
      "[75]\ttrain-mlogloss:2.89263\teval-mlogloss:2.89991\n",
      "[76]\ttrain-mlogloss:2.89259\teval-mlogloss:2.89988\n",
      "[77]\ttrain-mlogloss:2.89255\teval-mlogloss:2.89984\n",
      "[78]\ttrain-mlogloss:2.89251\teval-mlogloss:2.89981\n",
      "[79]\ttrain-mlogloss:2.89247\teval-mlogloss:2.89978\n",
      "[80]\ttrain-mlogloss:2.89243\teval-mlogloss:2.89975\n",
      "[81]\ttrain-mlogloss:2.89239\teval-mlogloss:2.89972\n",
      "[82]\ttrain-mlogloss:2.89236\teval-mlogloss:2.89968\n",
      "[83]\ttrain-mlogloss:2.89232\teval-mlogloss:2.89966\n",
      "[84]\ttrain-mlogloss:2.89228\teval-mlogloss:2.89963\n",
      "[85]\ttrain-mlogloss:2.89225\teval-mlogloss:2.8996\n",
      "[86]\ttrain-mlogloss:2.89221\teval-mlogloss:2.89957\n",
      "[87]\ttrain-mlogloss:2.89218\teval-mlogloss:2.89954\n",
      "[88]\ttrain-mlogloss:2.89214\teval-mlogloss:2.89952\n",
      "[89]\ttrain-mlogloss:2.89211\teval-mlogloss:2.89949\n",
      "[90]\ttrain-mlogloss:2.89208\teval-mlogloss:2.89946\n",
      "[91]\ttrain-mlogloss:2.89205\teval-mlogloss:2.89944\n",
      "[92]\ttrain-mlogloss:2.89202\teval-mlogloss:2.89941\n",
      "[93]\ttrain-mlogloss:2.89199\teval-mlogloss:2.89939\n",
      "[94]\ttrain-mlogloss:2.89195\teval-mlogloss:2.89936\n",
      "[95]\ttrain-mlogloss:2.89193\teval-mlogloss:2.89934\n",
      "[96]\ttrain-mlogloss:2.89189\teval-mlogloss:2.89932\n",
      "[97]\ttrain-mlogloss:2.89187\teval-mlogloss:2.8993\n",
      "[98]\ttrain-mlogloss:2.89184\teval-mlogloss:2.89927\n",
      "[99]\ttrain-mlogloss:2.89181\teval-mlogloss:2.89925\n",
      "[100]\ttrain-mlogloss:2.89178\teval-mlogloss:2.89923\n",
      "[101]\ttrain-mlogloss:2.89176\teval-mlogloss:2.89921\n",
      "[102]\ttrain-mlogloss:2.89173\teval-mlogloss:2.89919\n",
      "[103]\ttrain-mlogloss:2.8917\teval-mlogloss:2.89917\n",
      "[104]\ttrain-mlogloss:2.89168\teval-mlogloss:2.89915\n",
      "[105]\ttrain-mlogloss:2.89165\teval-mlogloss:2.89913\n",
      "[106]\ttrain-mlogloss:2.89163\teval-mlogloss:2.89911\n",
      "[107]\ttrain-mlogloss:2.8916\teval-mlogloss:2.89909\n",
      "[108]\ttrain-mlogloss:2.89158\teval-mlogloss:2.89907\n",
      "[109]\ttrain-mlogloss:2.89155\teval-mlogloss:2.89905\n",
      "[110]\ttrain-mlogloss:2.89153\teval-mlogloss:2.89904\n",
      "[111]\ttrain-mlogloss:2.89151\teval-mlogloss:2.89902\n",
      "[112]\ttrain-mlogloss:2.89148\teval-mlogloss:2.899\n",
      "[113]\ttrain-mlogloss:2.89146\teval-mlogloss:2.89898\n",
      "[114]\ttrain-mlogloss:2.89144\teval-mlogloss:2.89897\n",
      "[115]\ttrain-mlogloss:2.89142\teval-mlogloss:2.89895\n",
      "[116]\ttrain-mlogloss:2.89139\teval-mlogloss:2.89893\n",
      "[117]\ttrain-mlogloss:2.89137\teval-mlogloss:2.89892\n",
      "[118]\ttrain-mlogloss:2.89135\teval-mlogloss:2.8989\n",
      "[119]\ttrain-mlogloss:2.89133\teval-mlogloss:2.89889\n",
      "[120]\ttrain-mlogloss:2.89131\teval-mlogloss:2.89887\n",
      "[121]\ttrain-mlogloss:2.89129\teval-mlogloss:2.89886\n",
      "[122]\ttrain-mlogloss:2.89127\teval-mlogloss:2.89884\n",
      "[123]\ttrain-mlogloss:2.89125\teval-mlogloss:2.89883\n",
      "[124]\ttrain-mlogloss:2.89123\teval-mlogloss:2.89882\n",
      "[125]\ttrain-mlogloss:2.89121\teval-mlogloss:2.8988\n",
      "[126]\ttrain-mlogloss:2.89119\teval-mlogloss:2.89879\n",
      "[127]\ttrain-mlogloss:2.89118\teval-mlogloss:2.89878\n",
      "[128]\ttrain-mlogloss:2.89116\teval-mlogloss:2.89877\n",
      "[129]\ttrain-mlogloss:2.89114\teval-mlogloss:2.89875\n",
      "[130]\ttrain-mlogloss:2.89112\teval-mlogloss:2.89874\n",
      "[131]\ttrain-mlogloss:2.8911\teval-mlogloss:2.89873\n",
      "[132]\ttrain-mlogloss:2.89109\teval-mlogloss:2.89872\n",
      "[133]\ttrain-mlogloss:2.89107\teval-mlogloss:2.89871\n",
      "[134]\ttrain-mlogloss:2.89105\teval-mlogloss:2.8987\n",
      "[135]\ttrain-mlogloss:2.89104\teval-mlogloss:2.89869\n",
      "[136]\ttrain-mlogloss:2.89102\teval-mlogloss:2.89868\n",
      "[137]\ttrain-mlogloss:2.891\teval-mlogloss:2.89867\n",
      "[138]\ttrain-mlogloss:2.89099\teval-mlogloss:2.89866\n",
      "[139]\ttrain-mlogloss:2.89097\teval-mlogloss:2.89865\n",
      "[140]\ttrain-mlogloss:2.89096\teval-mlogloss:2.89864\n",
      "[141]\ttrain-mlogloss:2.89094\teval-mlogloss:2.89863\n",
      "[142]\ttrain-mlogloss:2.89093\teval-mlogloss:2.89862\n",
      "[143]\ttrain-mlogloss:2.89091\teval-mlogloss:2.89861\n",
      "[144]\ttrain-mlogloss:2.8909\teval-mlogloss:2.8986\n",
      "[145]\ttrain-mlogloss:2.89088\teval-mlogloss:2.89859\n",
      "[146]\ttrain-mlogloss:2.89087\teval-mlogloss:2.89858\n",
      "[147]\ttrain-mlogloss:2.89085\teval-mlogloss:2.89858\n",
      "[148]\ttrain-mlogloss:2.89084\teval-mlogloss:2.89857\n",
      "[149]\ttrain-mlogloss:2.89082\teval-mlogloss:2.89856\n",
      "[150]\ttrain-mlogloss:2.89081\teval-mlogloss:2.89855\n",
      "[151]\ttrain-mlogloss:2.8908\teval-mlogloss:2.89854\n",
      "[152]\ttrain-mlogloss:2.89079\teval-mlogloss:2.89854\n",
      "[153]\ttrain-mlogloss:2.89077\teval-mlogloss:2.89853\n",
      "[154]\ttrain-mlogloss:2.89076\teval-mlogloss:2.89852\n",
      "[155]\ttrain-mlogloss:2.89075\teval-mlogloss:2.89852\n",
      "[156]\ttrain-mlogloss:2.89073\teval-mlogloss:2.89851\n",
      "[157]\ttrain-mlogloss:2.89072\teval-mlogloss:2.8985\n",
      "[158]\ttrain-mlogloss:2.89071\teval-mlogloss:2.8985\n",
      "[159]\ttrain-mlogloss:2.8907\teval-mlogloss:2.89849\n",
      "[160]\ttrain-mlogloss:2.89069\teval-mlogloss:2.89848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161]\ttrain-mlogloss:2.89067\teval-mlogloss:2.89848\n",
      "[162]\ttrain-mlogloss:2.89066\teval-mlogloss:2.89847\n",
      "[163]\ttrain-mlogloss:2.89065\teval-mlogloss:2.89846\n",
      "[164]\ttrain-mlogloss:2.89064\teval-mlogloss:2.89846\n",
      "[165]\ttrain-mlogloss:2.89063\teval-mlogloss:2.89845\n",
      "[166]\ttrain-mlogloss:2.89062\teval-mlogloss:2.89845\n",
      "[167]\ttrain-mlogloss:2.89061\teval-mlogloss:2.89844\n",
      "[168]\ttrain-mlogloss:2.8906\teval-mlogloss:2.89843\n",
      "[169]\ttrain-mlogloss:2.89059\teval-mlogloss:2.89843\n",
      "[170]\ttrain-mlogloss:2.89058\teval-mlogloss:2.89842\n",
      "[171]\ttrain-mlogloss:2.89057\teval-mlogloss:2.89841\n",
      "[172]\ttrain-mlogloss:2.89055\teval-mlogloss:2.89841\n",
      "[173]\ttrain-mlogloss:2.89054\teval-mlogloss:2.8984\n",
      "[174]\ttrain-mlogloss:2.89053\teval-mlogloss:2.89839\n",
      "[175]\ttrain-mlogloss:2.89052\teval-mlogloss:2.89839\n",
      "[176]\ttrain-mlogloss:2.89052\teval-mlogloss:2.89838\n",
      "[177]\ttrain-mlogloss:2.89051\teval-mlogloss:2.89837\n",
      "[178]\ttrain-mlogloss:2.8905\teval-mlogloss:2.89837\n",
      "[179]\ttrain-mlogloss:2.89049\teval-mlogloss:2.89836\n",
      "[180]\ttrain-mlogloss:2.89048\teval-mlogloss:2.89836\n",
      "[181]\ttrain-mlogloss:2.89047\teval-mlogloss:2.89835\n",
      "[182]\ttrain-mlogloss:2.89046\teval-mlogloss:2.89835\n",
      "[183]\ttrain-mlogloss:2.89045\teval-mlogloss:2.89834\n",
      "[184]\ttrain-mlogloss:2.89044\teval-mlogloss:2.89833\n",
      "[185]\ttrain-mlogloss:2.89044\teval-mlogloss:2.89833\n",
      "[186]\ttrain-mlogloss:2.89043\teval-mlogloss:2.89832\n",
      "[187]\ttrain-mlogloss:2.89042\teval-mlogloss:2.89832\n",
      "[188]\ttrain-mlogloss:2.89041\teval-mlogloss:2.89832\n",
      "[189]\ttrain-mlogloss:2.8904\teval-mlogloss:2.89831\n",
      "[190]\ttrain-mlogloss:2.89039\teval-mlogloss:2.89831\n",
      "[191]\ttrain-mlogloss:2.89039\teval-mlogloss:2.8983\n",
      "[192]\ttrain-mlogloss:2.89038\teval-mlogloss:2.8983\n",
      "[193]\ttrain-mlogloss:2.89037\teval-mlogloss:2.8983\n",
      "[194]\ttrain-mlogloss:2.89036\teval-mlogloss:2.89829\n",
      "[195]\ttrain-mlogloss:2.89036\teval-mlogloss:2.89829\n",
      "[196]\ttrain-mlogloss:2.89035\teval-mlogloss:2.89828\n",
      "[197]\ttrain-mlogloss:2.89034\teval-mlogloss:2.89828\n",
      "[198]\ttrain-mlogloss:2.89033\teval-mlogloss:2.89828\n",
      "[199]\ttrain-mlogloss:2.89033\teval-mlogloss:2.89827\n",
      "[200]\ttrain-mlogloss:2.89032\teval-mlogloss:2.89827\n",
      "[201]\ttrain-mlogloss:2.89031\teval-mlogloss:2.89827\n",
      "[202]\ttrain-mlogloss:2.89031\teval-mlogloss:2.89826\n",
      "[203]\ttrain-mlogloss:2.8903\teval-mlogloss:2.89826\n",
      "[204]\ttrain-mlogloss:2.89029\teval-mlogloss:2.89826\n",
      "[205]\ttrain-mlogloss:2.89028\teval-mlogloss:2.89825\n",
      "[206]\ttrain-mlogloss:2.89028\teval-mlogloss:2.89825\n",
      "[207]\ttrain-mlogloss:2.89027\teval-mlogloss:2.89825\n",
      "[208]\ttrain-mlogloss:2.89026\teval-mlogloss:2.89824\n",
      "[209]\ttrain-mlogloss:2.89026\teval-mlogloss:2.89824\n",
      "[210]\ttrain-mlogloss:2.89025\teval-mlogloss:2.89824\n",
      "[211]\ttrain-mlogloss:2.89025\teval-mlogloss:2.89824\n",
      "[212]\ttrain-mlogloss:2.89024\teval-mlogloss:2.89823\n",
      "[213]\ttrain-mlogloss:2.89023\teval-mlogloss:2.89823\n",
      "[214]\ttrain-mlogloss:2.89023\teval-mlogloss:2.89823\n",
      "[215]\ttrain-mlogloss:2.89022\teval-mlogloss:2.89823\n",
      "[216]\ttrain-mlogloss:2.89021\teval-mlogloss:2.89822\n",
      "[217]\ttrain-mlogloss:2.89021\teval-mlogloss:2.89822\n",
      "[218]\ttrain-mlogloss:2.8902\teval-mlogloss:2.89822\n",
      "[219]\ttrain-mlogloss:2.8902\teval-mlogloss:2.89822\n",
      "[220]\ttrain-mlogloss:2.89019\teval-mlogloss:2.89821\n",
      "[221]\ttrain-mlogloss:2.89019\teval-mlogloss:2.89821\n",
      "[222]\ttrain-mlogloss:2.89018\teval-mlogloss:2.89821\n",
      "[223]\ttrain-mlogloss:2.89018\teval-mlogloss:2.89821\n",
      "[224]\ttrain-mlogloss:2.89017\teval-mlogloss:2.89821\n",
      "[225]\ttrain-mlogloss:2.89017\teval-mlogloss:2.89821\n",
      "[226]\ttrain-mlogloss:2.89016\teval-mlogloss:2.8982\n",
      "[227]\ttrain-mlogloss:2.89016\teval-mlogloss:2.8982\n",
      "[228]\ttrain-mlogloss:2.89015\teval-mlogloss:2.8982\n",
      "[229]\ttrain-mlogloss:2.89015\teval-mlogloss:2.8982\n",
      "[230]\ttrain-mlogloss:2.89014\teval-mlogloss:2.8982\n",
      "[231]\ttrain-mlogloss:2.89014\teval-mlogloss:2.8982\n",
      "[232]\ttrain-mlogloss:2.89013\teval-mlogloss:2.8982\n",
      "[233]\ttrain-mlogloss:2.89013\teval-mlogloss:2.8982\n",
      "[234]\ttrain-mlogloss:2.89012\teval-mlogloss:2.8982\n",
      "[235]\ttrain-mlogloss:2.89012\teval-mlogloss:2.8982\n",
      "[236]\ttrain-mlogloss:2.89012\teval-mlogloss:2.8982\n",
      "[237]\ttrain-mlogloss:2.89011\teval-mlogloss:2.8982\n",
      "[238]\ttrain-mlogloss:2.89011\teval-mlogloss:2.8982\n",
      "[239]\ttrain-mlogloss:2.8901\teval-mlogloss:2.8982\n",
      "[240]\ttrain-mlogloss:2.8901\teval-mlogloss:2.8982\n",
      "[241]\ttrain-mlogloss:2.8901\teval-mlogloss:2.8982\n",
      "[242]\ttrain-mlogloss:2.89009\teval-mlogloss:2.8982\n",
      "[243]\ttrain-mlogloss:2.89009\teval-mlogloss:2.8982\n",
      "[244]\ttrain-mlogloss:2.89008\teval-mlogloss:2.8982\n",
      "[245]\ttrain-mlogloss:2.89008\teval-mlogloss:2.8982\n",
      "[246]\ttrain-mlogloss:2.89007\teval-mlogloss:2.8982\n",
      "[247]\ttrain-mlogloss:2.89007\teval-mlogloss:2.8982\n",
      "[248]\ttrain-mlogloss:2.89007\teval-mlogloss:2.89821\n",
      "[249]\ttrain-mlogloss:2.89006\teval-mlogloss:2.89821\n",
      "[250]\ttrain-mlogloss:2.89006\teval-mlogloss:2.89821\n",
      "[251]\ttrain-mlogloss:2.89006\teval-mlogloss:2.89821\n",
      "[252]\ttrain-mlogloss:2.89005\teval-mlogloss:2.89821\n",
      "[253]\ttrain-mlogloss:2.89005\teval-mlogloss:2.89821\n",
      "[254]\ttrain-mlogloss:2.89005\teval-mlogloss:2.89821\n",
      "[255]\ttrain-mlogloss:2.89004\teval-mlogloss:2.89821\n",
      "[256]\ttrain-mlogloss:2.89004\teval-mlogloss:2.89822\n",
      "[257]\ttrain-mlogloss:2.89004\teval-mlogloss:2.89822\n",
      "[258]\ttrain-mlogloss:2.89003\teval-mlogloss:2.89822\n",
      "[259]\ttrain-mlogloss:2.89003\teval-mlogloss:2.89822\n",
      "Stopping. Best iteration:\n",
      "[234]\ttrain-mlogloss:2.89012\teval-mlogloss:2.8982\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-06 09:13:42,855 - DEBUG - data_preprocessing.py:217 - Index(['device_id', 'type_no_w', '01', '02', '02f', '1100', '1105', '1107',\n",
      "       '1s', '2013022',\n",
      "       ...\n",
      "       '零售o2o', '音乐学习', '音乐播放器', '音乐编辑', '音乐节奏', '项目管理', '飞行射击', '食谱', '餐饮品牌',\n",
      "       '驾驶学习'],\n",
      "      dtype='object', length=872)\n",
      "2018-10-06 09:13:42,856 - DEBUG - data_preprocessing.py:218 - (22727, 872)\n",
      "2018-10-06 09:13:42,900 - DEBUG - data_preprocessing.py:231 - Index(['type_no_w', '01', '02', '02f', '1100', '1105', '1107', '1s', '2013022',\n",
      "       '2013023',\n",
      "       ...\n",
      "       '零售o2o', '音乐学习', '音乐播放器', '音乐编辑', '音乐节奏', '项目管理', '飞行射击', '食谱', '餐饮品牌',\n",
      "       '驾驶学习'],\n",
      "      dtype='object', length=871)\n",
      "2018-10-06 09:13:42,902 - DEBUG - data_preprocessing.py:232 - (22727, 871)\n",
      "2018-10-06 09:13:42,903 - DEBUG - data_preprocessing.py:233 -    type_no_w   01   02  02f  1100  1105  1107   1s  2013022  2013023  ...   \\\n",
      "0   0.613990  0.0  0.0  0.0   0.0   0.0   0.0  0.0      0.0      0.0  ...    \n",
      "1   0.639269  0.0  0.0  0.0   0.0   0.0   0.0  0.0      0.0      0.0  ...    \n",
      "\n",
      "   零售o2o  音乐学习  音乐播放器  音乐编辑  音乐节奏  项目管理  飞行射击  食谱  餐饮品牌  驾驶学习  \n",
      "0      0     0      0     0     0     0     0   0     0     0  \n",
      "1      0     0      0     0     0     0     0   0     0     0  \n",
      "\n",
      "[2 rows x 871 columns]\n",
      "2018-10-06 09:13:44,597 - DEBUG - <ipython-input-3-7252c1eb9d58>:16 -             1-0       1-1       1-2       1-3       1-4       1-5       1-6  \\\n",
      "0      0.002221  0.017205  0.042347  0.073232  0.074948  0.089549  0.097996   \n",
      "1      0.002225  0.017234  0.042651  0.073725  0.075078  0.089132  0.097813   \n",
      "2      0.002136  0.016543  0.036436  0.063585  0.072067  0.098184  0.101430   \n",
      "3      0.002158  0.016713  0.037821  0.065857  0.072807  0.096067  0.100647   \n",
      "4      0.002158  0.016713  0.037821  0.065857  0.072807  0.096067  0.100647   \n",
      "5      0.002204  0.017068  0.040997  0.071040  0.074351  0.091433  0.098800   \n",
      "6      0.002158  0.016713  0.037821  0.065857  0.072807  0.096067  0.100647   \n",
      "7      0.002136  0.016543  0.036436  0.063585  0.072067  0.098184  0.101430   \n",
      "8      0.002158  0.016713  0.037821  0.065857  0.072807  0.096067  0.100647   \n",
      "9      0.002320  0.017975  0.045686  0.070854  0.076243  0.088046  0.097164   \n",
      "10     0.002211  0.017129  0.041594  0.072009  0.074619  0.090595  0.098446   \n",
      "11     0.002223  0.017227  0.038572  0.060290  0.073024  0.097956  0.101206   \n",
      "12     0.002136  0.016543  0.036436  0.063585  0.072067  0.098184  0.101430   \n",
      "13     0.002211  0.017129  0.041594  0.072009  0.074619  0.090595  0.098446   \n",
      "14     0.002136  0.016543  0.036436  0.063585  0.072067  0.098184  0.101430   \n",
      "15     0.002136  0.016543  0.036436  0.063585  0.072067  0.098184  0.101430   \n",
      "16     0.002229  0.017264  0.042956  0.074220  0.075207  0.088715  0.097630   \n",
      "17     0.002127  0.016474  0.035892  0.062691  0.071765  0.099032  0.101733   \n",
      "18     0.002158  0.016713  0.037821  0.065857  0.072807  0.096067  0.100647   \n",
      "19     0.002241  0.017358  0.042682  0.071742  0.075076  0.089898  0.098111   \n",
      "20     0.002136  0.016543  0.036436  0.063585  0.072067  0.098184  0.101430   \n",
      "21     0.002127  0.016474  0.035892  0.062691  0.071765  0.099032  0.101733   \n",
      "22     0.002150  0.016653  0.036518  0.062120  0.072074  0.098790  0.101622   \n",
      "23     0.002235  0.017312  0.042225  0.071021  0.074878  0.090526  0.098381   \n",
      "24     0.002136  0.016543  0.036436  0.063585  0.072067  0.098184  0.101430   \n",
      "25     0.002150  0.016653  0.036518  0.062120  0.072074  0.098790  0.101622   \n",
      "26     0.002342  0.018149  0.046622  0.070761  0.076584  0.087402  0.096831   \n",
      "27     0.002136  0.016543  0.036436  0.063585  0.072067  0.098184  0.101430   \n",
      "28     0.002218  0.017187  0.040240  0.066516  0.073951  0.093898  0.099745   \n",
      "29     0.002229  0.017264  0.042956  0.074220  0.075207  0.088715  0.097630   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "22697  0.002129  0.016491  0.036027  0.062914  0.071841  0.098820  0.101658   \n",
      "22698  0.002211  0.017129  0.041594  0.072009  0.074619  0.090595  0.098446   \n",
      "22699  0.002227  0.017249  0.042804  0.073972  0.075142  0.088923  0.097722   \n",
      "22700  0.002129  0.016491  0.036027  0.062914  0.071841  0.098820  0.101658   \n",
      "22701  0.002129  0.016491  0.036027  0.062914  0.071841  0.098820  0.101658   \n",
      "22702  0.002136  0.016543  0.036436  0.063585  0.072067  0.098184  0.101430   \n",
      "22703  0.002204  0.017068  0.040997  0.071040  0.074351  0.091433  0.098800   \n",
      "22704  0.002235  0.017312  0.042225  0.071021  0.074878  0.090526  0.098381   \n",
      "22705  0.002204  0.017068  0.040997  0.071040  0.074351  0.091433  0.098800   \n",
      "22706  0.002223  0.017220  0.042499  0.073478  0.075013  0.089341  0.097905   \n",
      "22707  0.002229  0.017264  0.042956  0.074220  0.075207  0.088715  0.097630   \n",
      "22708  0.002127  0.016474  0.035892  0.062691  0.071765  0.099032  0.101733   \n",
      "22709  0.002235  0.017312  0.042225  0.071021  0.074878  0.090526  0.098381   \n",
      "22710  0.002229  0.017264  0.042956  0.074220  0.075207  0.088715  0.097630   \n",
      "22711  0.002136  0.016543  0.036436  0.063585  0.072067  0.098184  0.101430   \n",
      "22712  0.002211  0.017129  0.041594  0.072009  0.074619  0.090595  0.098446   \n",
      "22713  0.002330  0.018059  0.046239  0.071113  0.076450  0.087544  0.096922   \n",
      "22714  0.002229  0.017264  0.042956  0.074220  0.075207  0.088715  0.097630   \n",
      "22715  0.002320  0.017975  0.045686  0.070854  0.076243  0.088046  0.097164   \n",
      "22716  0.002223  0.017220  0.042499  0.073478  0.075013  0.089341  0.097905   \n",
      "22717  0.002225  0.017234  0.042651  0.073725  0.075078  0.089132  0.097813   \n",
      "22718  0.002127  0.016474  0.035892  0.062691  0.071765  0.099032  0.101733   \n",
      "22719  0.002326  0.018023  0.046086  0.071254  0.076396  0.087600  0.096958   \n",
      "22720  0.002229  0.017264  0.042956  0.074220  0.075207  0.088715  0.097630   \n",
      "22721  0.002127  0.016474  0.035892  0.062691  0.071765  0.099032  0.101733   \n",
      "22722  0.002223  0.017220  0.042499  0.073478  0.075013  0.089341  0.097905   \n",
      "22723  0.002129  0.016491  0.036027  0.062914  0.071841  0.098820  0.101658   \n",
      "22724  0.002158  0.016713  0.037821  0.065857  0.072807  0.096067  0.100647   \n",
      "22725  0.002326  0.018023  0.046086  0.071254  0.076396  0.087600  0.096958   \n",
      "22726  0.002129  0.016491  0.036027  0.062914  0.071841  0.098820  0.101658   \n",
      "\n",
      "            1-7       1-8       1-9    ...          2-1       2-2       2-3  \\\n",
      "0      0.077494  0.057932  0.037719    ...     0.020559  0.031223  0.037690   \n",
      "1      0.077152  0.057827  0.037621    ...     0.020649  0.031451  0.037923   \n",
      "2      0.084531  0.059902  0.039626    ...     0.018734  0.026798  0.033096   \n",
      "3      0.082812  0.059454  0.039178    ...     0.019176  0.027834  0.034184   \n",
      "4      0.082812  0.059454  0.039178    ...     0.019176  0.027834  0.034184   \n",
      "5      0.079035  0.058394  0.038153    ...     0.020156  0.030212  0.036652   \n",
      "6      0.082812  0.059454  0.039178    ...     0.019176  0.027834  0.034184   \n",
      "7      0.084531  0.059902  0.039626    ...     0.018734  0.026798  0.033096   \n",
      "8      0.082812  0.059454  0.039178    ...     0.019176  0.027834  0.034184   \n",
      "9      0.078281  0.057160  0.037764    ...     0.021526  0.032786  0.038375   \n",
      "10     0.078350  0.058191  0.037961    ...     0.020335  0.030658  0.037112   \n",
      "11     0.086638  0.059462  0.040022    ...     0.019398  0.027585  0.033119   \n",
      "12     0.084531  0.059902  0.039626    ...     0.018734  0.026798  0.033096   \n",
      "13     0.078350  0.058191  0.037961    ...     0.020335  0.030658  0.037112   \n",
      "14     0.084531  0.059902  0.039626    ...     0.018734  0.026798  0.033096   \n",
      "15     0.084531  0.059902  0.039626    ...     0.018734  0.026798  0.033096   \n",
      "16     0.076810  0.057721  0.037523    ...     0.020739  0.031680  0.038157   \n",
      "17     0.085219  0.060075  0.039803    ...     0.018558  0.026392  0.032666   \n",
      "18     0.082812  0.059454  0.039178    ...     0.019176  0.027834  0.034184   \n",
      "19     0.078327  0.057919  0.037910    ...     0.020658  0.031236  0.037469   \n",
      "20     0.084531  0.059902  0.039626    ...     0.018734  0.026798  0.033096   \n",
      "21     0.085219  0.060075  0.039803    ...     0.018558  0.026392  0.032666   \n",
      "22     0.085567  0.059938  0.039861    ...     0.018758  0.026675  0.032779   \n",
      "23     0.078845  0.058074  0.038056    ...     0.020523  0.030896  0.037122   \n",
      "24     0.084531  0.059902  0.039626    ...     0.018734  0.026798  0.033096   \n",
      "25     0.085567  0.059938  0.039861    ...     0.018758  0.026675  0.032779   \n",
      "26     0.078136  0.056912  0.037683    ...     0.021790  0.033285  0.038691   \n",
      "27     0.084531  0.059902  0.039626    ...     0.018734  0.026798  0.033096   \n",
      "28     0.082036  0.058798  0.038902    ...     0.019924  0.029258  0.035284   \n",
      "29     0.076810  0.057721  0.037523    ...     0.020739  0.031680  0.038157   \n",
      "...         ...       ...       ...    ...          ...       ...       ...   \n",
      "22697  0.085047  0.060032  0.039759    ...     0.018602  0.026493  0.032773   \n",
      "22698  0.078350  0.058191  0.037961    ...     0.020335  0.030658  0.037112   \n",
      "22699  0.076981  0.057774  0.037572    ...     0.020694  0.031566  0.038040   \n",
      "22700  0.085047  0.060032  0.039759    ...     0.018602  0.026493  0.032773   \n",
      "22701  0.085047  0.060032  0.039759    ...     0.018602  0.026493  0.032773   \n",
      "22702  0.084531  0.059902  0.039626    ...     0.018734  0.026798  0.033096   \n",
      "22703  0.079035  0.058394  0.038153    ...     0.020156  0.030212  0.036652   \n",
      "22704  0.078845  0.058074  0.038056    ...     0.020523  0.030896  0.037122   \n",
      "22705  0.079035  0.058394  0.038153    ...     0.020156  0.030212  0.036652   \n",
      "22706  0.077323  0.057880  0.037670    ...     0.020604  0.031337  0.037806   \n",
      "22707  0.076810  0.057721  0.037523    ...     0.020739  0.031680  0.038157   \n",
      "22708  0.085219  0.060075  0.039803    ...     0.018558  0.026392  0.032666   \n",
      "22709  0.078845  0.058074  0.038056    ...     0.020523  0.030896  0.037122   \n",
      "22710  0.076810  0.057721  0.037523    ...     0.020739  0.031680  0.038157   \n",
      "22711  0.084531  0.059902  0.039626    ...     0.018734  0.026798  0.033096   \n",
      "22712  0.078350  0.058191  0.037961    ...     0.020335  0.030658  0.037112   \n",
      "22713  0.078007  0.057000  0.037671    ...     0.021683  0.033121  0.038640   \n",
      "22714  0.076810  0.057721  0.037523    ...     0.020739  0.031680  0.038157   \n",
      "22715  0.078281  0.057160  0.037764    ...     0.021526  0.032786  0.038375   \n",
      "22716  0.077323  0.057880  0.037670    ...     0.020604  0.031337  0.037806   \n",
      "22717  0.077152  0.057827  0.037621    ...     0.020649  0.031451  0.037923   \n",
      "22718  0.085219  0.060075  0.039803    ...     0.018558  0.026392  0.032666   \n",
      "22719  0.077955  0.057035  0.037665    ...     0.021640  0.033055  0.038619   \n",
      "22720  0.076810  0.057721  0.037523    ...     0.020739  0.031680  0.038157   \n",
      "22721  0.085219  0.060075  0.039803    ...     0.018558  0.026392  0.032666   \n",
      "22722  0.077323  0.057880  0.037670    ...     0.020604  0.031337  0.037806   \n",
      "22723  0.085047  0.060032  0.039759    ...     0.018602  0.026493  0.032773   \n",
      "22724  0.082812  0.059454  0.039178    ...     0.019176  0.027834  0.034184   \n",
      "22725  0.077955  0.057035  0.037665    ...     0.021640  0.033055  0.038619   \n",
      "22726  0.085047  0.060032  0.039759    ...     0.018602  0.026493  0.032773   \n",
      "\n",
      "            2-4       2-5       2-6       2-7       2-8       2-9      2-10  \n",
      "0      0.046107  0.060359  0.055786  0.037826  0.027543  0.016181  0.022891  \n",
      "1      0.046239  0.060351  0.055727  0.037754  0.027590  0.016209  0.022900  \n",
      "2      0.043319  0.060299  0.056799  0.039176  0.026484  0.015559  0.022631  \n",
      "3      0.044012  0.060353  0.056587  0.038868  0.026756  0.015719  0.022709  \n",
      "4      0.044012  0.060353  0.056587  0.038868  0.026756  0.015719  0.022709  \n",
      "5      0.045508  0.060382  0.056041  0.038141  0.027323  0.016053  0.022847  \n",
      "6      0.044012  0.060353  0.056587  0.038868  0.026756  0.015719  0.022709  \n",
      "7      0.043319  0.060299  0.056799  0.039176  0.026484  0.015559  0.022631  \n",
      "8      0.044012  0.060353  0.056587  0.038868  0.026756  0.015719  0.022709  \n",
      "9      0.045721  0.058220  0.055558  0.039356  0.028761  0.016898  0.024834  \n",
      "10     0.045775  0.060374  0.055930  0.038002  0.027422  0.016110  0.022868  \n",
      "11     0.042615  0.058185  0.056793  0.041072  0.027565  0.016195  0.024594  \n",
      "12     0.043319  0.060299  0.056799  0.039176  0.026484  0.015559  0.022631  \n",
      "13     0.045775  0.060374  0.055930  0.038002  0.027422  0.016110  0.022868  \n",
      "14     0.043319  0.060299  0.056799  0.039176  0.026484  0.015559  0.022631  \n",
      "15     0.043319  0.060299  0.056799  0.039176  0.026484  0.015559  0.022631  \n",
      "16     0.046371  0.060342  0.055667  0.037682  0.027638  0.016237  0.022908  \n",
      "17     0.043040  0.060270  0.056878  0.039296  0.026373  0.015494  0.022597  \n",
      "18     0.044012  0.060353  0.056587  0.038868  0.026756  0.015719  0.022709  \n",
      "19     0.045779  0.059801  0.055838  0.038365  0.027784  0.016323  0.023385  \n",
      "20     0.043319  0.060299  0.056799  0.039176  0.026484  0.015559  0.022631  \n",
      "21     0.043040  0.060270  0.056878  0.039296  0.026373  0.015494  0.022597  \n",
      "22     0.042945  0.059776  0.056866  0.039718  0.026656  0.015661  0.023061  \n",
      "23     0.045581  0.059811  0.055925  0.038472  0.027711  0.016280  0.023371  \n",
      "24     0.043319  0.060299  0.056799  0.039176  0.026484  0.015559  0.022631  \n",
      "25     0.042945  0.059776  0.056866  0.039718  0.026656  0.015661  0.023061  \n",
      "26     0.045739  0.057789  0.055451  0.039591  0.029038  0.017061  0.025233  \n",
      "27     0.043319  0.060299  0.056799  0.039176  0.026484  0.015559  0.022631  \n",
      "28     0.044354  0.059415  0.056355  0.039385  0.027508  0.016161  0.023650  \n",
      "29     0.046371  0.060342  0.055667  0.037682  0.027638  0.016237  0.022908  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "22697  0.043110  0.060277  0.056858  0.039266  0.026401  0.015511  0.022605  \n",
      "22698  0.045775  0.060374  0.055930  0.038002  0.027422  0.016110  0.022868  \n",
      "22699  0.046305  0.060346  0.055697  0.037718  0.027614  0.016223  0.022904  \n",
      "22700  0.043110  0.060277  0.056858  0.039266  0.026401  0.015511  0.022605  \n",
      "22701  0.043110  0.060277  0.056858  0.039266  0.026401  0.015511  0.022605  \n",
      "22702  0.043319  0.060299  0.056799  0.039176  0.026484  0.015559  0.022631  \n",
      "22703  0.045508  0.060382  0.056041  0.038141  0.027323  0.016053  0.022847  \n",
      "22704  0.045581  0.059811  0.055925  0.038472  0.027711  0.016280  0.023371  \n",
      "22705  0.045508  0.060382  0.056041  0.038141  0.027323  0.016053  0.022847  \n",
      "22706  0.046173  0.060355  0.055757  0.037790  0.027567  0.016196  0.022895  \n",
      "22707  0.046371  0.060342  0.055667  0.037682  0.027638  0.016237  0.022908  \n",
      "22708  0.043040  0.060270  0.056878  0.039296  0.026373  0.015494  0.022597  \n",
      "22709  0.045581  0.059811  0.055925  0.038472  0.027711  0.016280  0.023371  \n",
      "22710  0.046371  0.060342  0.055667  0.037682  0.027638  0.016237  0.022908  \n",
      "22711  0.043319  0.060299  0.056799  0.039176  0.026484  0.015559  0.022631  \n",
      "22712  0.045775  0.060374  0.055930  0.038002  0.027422  0.016110  0.022868  \n",
      "22713  0.045807  0.058050  0.055478  0.039396  0.028895  0.016977  0.024988  \n",
      "22714  0.046371  0.060342  0.055667  0.037682  0.027638  0.016237  0.022908  \n",
      "22715  0.045721  0.058220  0.055558  0.039356  0.028761  0.016898  0.024834  \n",
      "22716  0.046173  0.060355  0.055757  0.037790  0.027567  0.016196  0.022895  \n",
      "22717  0.046239  0.060351  0.055727  0.037754  0.027590  0.016209  0.022900  \n",
      "22718  0.043040  0.060270  0.056878  0.039296  0.026373  0.015494  0.022597  \n",
      "22719  0.045835  0.058155  0.055489  0.039317  0.028837  0.016943  0.024890  \n",
      "22720  0.046371  0.060342  0.055667  0.037682  0.027638  0.016237  0.022908  \n",
      "22721  0.043040  0.060270  0.056878  0.039296  0.026373  0.015494  0.022597  \n",
      "22722  0.046173  0.060355  0.055757  0.037790  0.027567  0.016196  0.022895  \n",
      "22723  0.043110  0.060277  0.056858  0.039266  0.026401  0.015511  0.022605  \n",
      "22724  0.044012  0.060353  0.056587  0.038868  0.026756  0.015719  0.022709  \n",
      "22725  0.045835  0.058155  0.055489  0.039317  0.028837  0.016943  0.024890  \n",
      "22726  0.043110  0.060277  0.056858  0.039266  0.026401  0.015511  0.022605  \n",
      "\n",
      "[22727 rows x 22 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-06 09:13:44,625 - DEBUG - <ipython-input-3-7252c1eb9d58>:18 - (22727,)\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.train(params, d_train, 1000, watchlist, early_stopping_rounds=25)\n",
    "X_test = gdbt_data_get_test()\n",
    "dtrain_predprob = clf.predict(xgb.DMatrix(X_test))\n",
    "\n",
    "columns=[]\n",
    "for i in [1,2]:\n",
    "    for j in range(11):\n",
    "        columns.append(str(i)+'-'+str(j))\n",
    "y_pred=pd.DataFrame(dtrain_predprob,columns=columns)\n",
    "def c(line):\n",
    "    return [round(x,6) for x in line]\n",
    "y_pred.apply(lambda line:c(line),axis=1)\n",
    "\n",
    "\n",
    "#            y_pred=np.array(y_pred).reshape(-1,1)\n",
    "logging.debug(y_pred)\n",
    "test_id=pd.read_csv(FLAGS.file_path+'deviceid_test.csv')\n",
    "logging.debug(test_id['device_id'].shape)\n",
    "test_id['device_id']=test_id['device_id'].map(str)\n",
    "test_id.rename(columns={'device_id':'DeviceID'}, inplace = True)\n",
    "fin=pd.concat([test_id,y_pred],axis=1)\n",
    "fin.to_csv(FLAGS.tmp_data_path+'1-gblinear.test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
